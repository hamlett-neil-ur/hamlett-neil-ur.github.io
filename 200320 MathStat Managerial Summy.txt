

                <p class="mb-5"><font color = "#003459"><strong><i>Managerial summary</strong></i></font>.  Communications between technologies and managers is perpetually challenging. This is particularly true for quantitative fields of artifical intelligence (AI) and data science. A November 2019 <a href="https://www.mckinsey.com/featured-insights/artificial-intelligence/global-ai-survey-ai-proves-its-worth-but-few-scale-impact">Global AI Survey</a> by McKinsey identifies model explainability as a key risk in adoptoin of AI-based technologies.  Babson College professor Thomas Davenport, a regular contributor to <i>Harvard Business Review</i> offers managers specific guidance in <a href="https://hbr.org/2013/07/keep-up-with-your-quants">"keeping up with your quants"</a>.   </p>

                <p class="mb-5">There are two aspects of interpreting data-science models are particularly important.  Clarifying the input factors that most-strongly influence a model's output is the first consideration.  The residual uncertainty from the model's output receives less attention. The work in this project develops new ways to communicate the left-over uncertainty from AI or data-science outputs.  It also provides the quant with a specific way to clearly understand the best left-over uncertainty that will be in the model output. </p>